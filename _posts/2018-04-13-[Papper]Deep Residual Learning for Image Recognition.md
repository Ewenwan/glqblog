---
    author: LuckyGong
    comments: true
    date: 2018-04-13 20:27
    layout: post
    title: 1
    categories:
    tags:
---

# 摘要

- Motivation：DNN难以训练
- 方法：根据其输入来学习残差函数而非原始函数（unreferenced functions）。
- 优点：易优化、更准确。

# 1.简介

## 1.1Motivation

- DNN可以通过增加深度来丰富特征的等级。
- 问题1：增加深度会引起梯度消失/爆炸阻碍收敛——初始归一化（normalized initialization）、中间归一化（intermediate normalization）
- 问题2：增加深度会引起准确率达到饱和后迅速退化（非过拟合）——用**恒等映射**（identity mapping）来构建增加的“多余的”层，其它“不多余”层直接从浅层模型中复制而来。但是目前无法找到一个与这种构建的解决方案相当或者更好的方案。

## 1.2解决——引入深度残差学习框架

- H(x)：底层映射，待拟合映射。F(x)：残差函数。
- 我们让神经网络的层来拟合**残差映射**（residual mapping），而不是让每一个堆叠的层直接来拟合所需的底层映射（underlying mapping）。
- 假设想要拟合的函数为H(x)，我们用堆叠的非线性层网络去拟合另一个函数F(x)=H(x)−x，从而想要学习的函数可以表示成H(x)=F(x)+x。假如在极端情况下，恒等映射是最优的（即我们学习的函数就是一个恒等函数），那么，将残差逼近到0(F(x)=0)相比于只用堆叠非线性层的网络去拟合恒等映射(H(x)=x)要更容易。

![](http://img.mp.sohu.com/upload/20170809/ebdf4d9c6cde43c0a3c26a398c10dc1e_th.png)

# 2.相关工作

## 2.1残差表示（Residual Representations）

- 表达：
  - VLAD：残差向量对应于字典进行编码的一种表达形式
  - Fisher Vector：VLAD的概率版本
- 求解偏微分方程（PDEs）：
  - 多重网格（**Multigrid**）法：将系统重新表达成多尺度的子问题来解决，每一个子问题就是解决粗细尺度之间的**残差问题**。
  - 分层基预处理：替代多重网格法，依赖于代表着两个尺度之间**残差向量**的变量。
- 实验证明 这些求解器比其他标准求解器的收敛要快得多，却并没有意识到这是该方法的残差特性所致。

## 2.2Shortcut连接

- [Szegedy2015Going](https://arxiv.org/abs/1409.4842)及[Lee2015deeply](https://arxiv.org/abs/1409.5185)：将一些中间层直接与辅助分类器相连接可以解决梯度消失/爆炸问题
-  [Szegedy2015Going](https://arxiv.org/abs/1409.4842)：一个“inception”层由一个shortcut分支和一些更深的分支组合而成。
- highway networks：将shortcut与门控函数结合起来。这些门是数据相关并且是有额外参数的，而我们的恒等shortcuts是无参数的。当一个门的shortcut是“closed”（接近于0）时，highway网络中的层表示非残差函数。相反的，我们的模型总是学习残差函数；我们的恒等shortcuts从不关闭，在学习额外的残差函数时，所有的信息总是通过的。此外，highway网络并不能由增加层的深度（例如， 超过100层）来提高准确率。



# 3.深度残差学习

## 3.1残差学习

- 多个非线性层能够逼近复杂的函数，这就等价于这些层能够逼近复杂的残差函数
- 学习F(x):=H(x)-x而不是H(x)，由此原始函数变成了H(x)=F(x) + x。学习F(x)而不是学习H(x)，更简单。

## 3.2**Identity Mapping by Shortcuts** 

- 形式化定义块：
  - x和F维度相同时，y = F(x; {Wi}) + x
  - x和F维度不同时，y=F{x;{Wi}} + Wsx
- 本文实验中涉及到的函数F是两层或者三层的，当然更多层也是可行的。但是如果F只含有一层，就和线性函数：y=W1x+x一致，因此并不具有任何优势。
- 我们还发现不仅是对于全连接层，对于卷积层也是同样适用的。函数F(x,{Wi})可以表示多个卷积层，在两个特征图的通道之间执行元素级的加法。

## 3.3网络结构

### 3.3.1Plain网络

- 受vgg网络启发。
- 卷积层主要为3*3的滤波器。
  - 输出特征尺寸相同的层含有相同数量的滤波器
  - 如果特征尺寸减半，则滤波器的数量增加一倍来保证每层的时间复杂度相同。
- 通过stride 为2的卷积层来进行下采样。
- 在网络的最后是一个全局的平均pooling层和一个1000 类的包含softmax的全连接层。
- 加权层的层数为34。
- 比VGG网络(Fig.3，左)有更少的滤波器和更低的计算复杂度。我们34层的结构含有36亿个FLOPs（乘-加），而这仅仅只有VGG-19 （196亿个FLOPs）的18%。

### 3.3.2残差网络

- 恒等shortcut：在以上plain网络的基础上，我们插入shortcut连接。如果输入和输出的维度相同时，可以直接使用恒等shortcuts。
- 映射shortcut：当维度增加时（虚线表示的shortcuts增加了维度），考虑两个选项，对于这两个选项，当shortcut跨越两种尺寸的特征图时，均使用stride为2的卷积： 
  - shortcut仍然使用恒等映射，在增加的维度上使用0来填充，这样做不会增加额外的参数
  - 使用第二个等式的映射shortcut来使维度保持一致（通过1*1的卷积）

# 4.实验

## 4.0数据预处理及调参

- 针对ImageNet的网络实现遵循了[Krizhevsky2012ImageNet](http://papers.nips.cc/paper/4824-imagenet-classification-withdeep-convolutional-neural-networks.pdf)和[Simonyan2014Very](https://arxiv.org/abs/1409.1556)。
- 调整图像的大小使它的短边长度随机的从[256,480][256,480] 中采样来增大图像的尺寸。 
- 从一张图像或者它的水平翻转图像中随机采样一个224*224的crop，每个像素都减去均值。
- 图像使用标准的颜色增强。我们在每一个卷积层之后，激活层之前均使用batch normalization（BN）。
- 我们根据[He2014spatial](https://www.computer.org/csdl/proceedings/iccv/2015/8391/00/8391b026-abs.html)来初始化权值然后从零开始训练所有plain/残差网络。
- 使用的mini-batch的尺寸为256。
- 学习率从0.1开始，每当错误率平稳时将学习率除以10，整个模型进行60∗10460∗104次迭代训练。我们将权值衰减设置为0.0001，a 动量为0.9。根据 [Ioffe2015Batch](https://arxiv.org/abs/1502.03167)，我们并没有使用Dropout。
- 在测试中，为了进行比较，我们采取标准的10-crop测试。 
- 为了达到最佳的结果，我们使用[Simonyan2014Very](https://arxiv.org/abs/1409.1556)及[He2014spatial](https://www.computer.org/csdl/proceedings/iccv/2015/8391/00/8391b026-abs.html)中的全卷积形式，并在多个尺度的结果上取平均分（调整图像的大小使它的短边长度分别为{224,256,384,480,640}）。

## 4.1ImageNet  

### 4.1.1Plain网络

- 18层和34层的plain网络，34层的网络比18层的网络具有更高的验证错误率。
- 这种优化上的困难不太可能是由梯度消失所造成的。因为这些plain网络的训练使用了BN，这能保证前向传递的信号是具有非零方差的。我们同样验证了在反向传递阶段的梯度由于BN而具有良好的范式，所以在前向和反向阶段的信号不会存在消失的问题。
- 我们推测，深层的plain网络的收敛率是指数衰减的，这可能会影响训练错误率的降低。这种优化困难的原因我们将在以后的工作中进行研究。

### 4.1.2恒等残差网络

- 实验：
  - 34层的ResNet比18层ResNet的结果更优(**2.8%**)。更重要的是，34 层的ResNet在训练集和验证集上均展现出了更低的错误率。
  - 与对应的plain网络相比，34层的ResNet在top-1 错误率上降低了**3.5%** (Table 2)，这得益于训练错误率的降低。
  - 18层的plain网络和残差网络的准确率很接近 ，但是ResNet 的收敛速度要快得多。
- 结论：
  - 表明了这种设置可以很好的解决退化问题，并且我们可以由增加的深度来提高准确率。
  - 验证了在极深的网络中残差学习的有效性。
  - 如果网络“**并不是特别深**” (如18层)，现有的SGD能够很好的对plain网络进行求解，而ResNet能够使优化得到更快的收敛。

### 4.1.3恒等 vs 映射Shortcuts

- 实验：
  - A：对增加的维度使用0填充，所有的shortcuts是无参数的
  - B：对增加的维度使用映射shortcuts，其它使用恒等shortcuts
  - C：所有的都是映射shortcuts
- 结论：
  - 表明了三种选项的模型都比对于的plain模型要好。
  - B略好于A，我们认为这是因为A中的0填充并没有进行残差学习。
  - C略好于B，我们把这个归结于更多的（13个）映射shortcuts所引入的参数。
  - 在A、B、C三个结果中细小的差距也表明了映射shortcuts对于解决退化问题**并不是必需的**。所以我们在本文接下来的内容中，为了**减少复杂度和模型尺寸**，并不使用选项C的模型。恒等shortcuts因其无额外复杂度而对以下介绍的**瓶颈结构**尤为重要。

### 4.1.4深度瓶颈结构

- 实验：
  - 考虑到训练时间的限制，我们将构建块修改成瓶颈的设计。
    - point 1：对于每一个残差函数F，我们使用了三个叠加层而不是两个。这三层分别是1 * 1、3 * 3 和1 * 1 的卷积：
      - 1 * 1 的层主要负责减少然后增加（恢复）维度
      - 3*3的层来减少输入和输出的维度。
    - point 2：无参数的恒等shortcuts对于瓶颈结构尤为重要。如果使用映射shortcuts来替代Fig.5(右)中的恒等shortcuts，将会发现时间复杂度和模型尺寸都会增加一倍，因为shortcut连接了两个高维端，所以恒等shortcuts对于瓶颈设计是更加有效的。
  - **50层 ResNet**：我们将34层网络中2层的模块替换成3层的瓶颈模块，整个模型也就变成了50层的ResNet 。对于增加的维度我们使用选项B来处理。整个模型含有38亿个FLOPs。
  - **101层和152层 ResNets**：我们使用更多的3层模块来构建101层和152层的ResNets (Table 1)。值得注意的是，虽然层的深度明显增加了，但是152层ResNet的计算复杂度(113亿个FLOPs)仍然比VGG-16(153 亿个FLOPs)和VGG-19(196亿个FLOPs）的小很多。
- 结论：50/101/152层ResNets比34层ResNet的准确率要高得多(Table 3 和4)。而且我们并没有观测到退化问题。所有的指标都证实了深度带来的好处。

## 4.2CIFAR-10

- 实验：
  - 网络的输入是32 * 32的减掉像素均值的图像。第一层是3 * 3的卷积层。然后我们使用6n个3*3的卷积层的堆叠，卷积层对应的特征图有三种：{32,16,8}{32,16,8}，每一种卷积层的数量为2n 个，对应的滤波器数量分别为{16,32,64}{16,32,64}。使用strde为2的卷积层进行下采样。在网络的最后是一个全局的平均pooling层和一个10类的包含softmax的全连接层。一共有6n+2个堆叠的加权层。
  - 使用shortcut连接3*3的卷积层对(共有 3n个shortcuts)。在这个数据集上我们所有的模型都使用恒等shortcuts(选项 A)，因此我们的残差模型和对应的plain模型具有相同的深度、宽度和参数量。
  - 权重的衰减设置为0.0001，动量为0.9，采用了[He2015Delving](http://ieeexplore.ieee.org/document/7410480/?reload=true&arnumber=7410480)中的权值初始化以及BN，但是不使用Dropout，mini-batch的大小为128，模型在2块GPU 上进行训练。学习率初始为0.1，在第32000和48000次迭代时将其除以10，总的迭代次数为64000，这是由45000/5000的训练集/验证集分配所决定的。我们在训练阶段遵循[Lee2015deeply](https://arxiv.org/abs/1409.5185)中的数据增强法则：在图像的每条边填充4个像素，然后在填充后的图像或者它的水平翻转图像上随机采样一个32*32 的crop。在测试阶段，我们只使用原始32 * 32的图像进行评估。
  - 本文中，我们并没有使用maxout/dropout，只是简单的通过设计深层窄模型来进行正则化，而且不用担心优化的难度。但是通过强大的正则化或许能够提高实验结果，我们会在以后进行研究。
- 结论：
  - 我们比较了n={3,5,7,9}，也就是20、32、44以及56层的网络。Fig.6(左) 展示了plain网络的结果。深度plain网络随着层数的加深，训练错误率也变大。这个现象与在ImageNet(Fig.4, 左)和MNIST上的结果很相似，表明了优化上的难度确实是一个很重要的问题。
  - Fig.6(中)展示了ResNets的效果。与ImageNet(Fig.4, 右)中类似，我们的ResNets能够很好的克服优化难题，并且随着深度加深，准确率也得到了提升。
  - 我们进一步探索了n=18，也就是110层的ResNet。当训练错误率在**80%**以下(大约400次迭代)之后，再将学习率调回0.1继续训练。剩余的学习和之前的一致。110层的ResNets很好的收敛了 (Fig.6, 中)。它与其他的深层窄模型，如FitNet和 Highway (Table 6)相比，具有更少的参数，然而却达到了最好的结果 (**6.43%**, Table 6)。
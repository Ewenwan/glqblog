---
    author: LuckyGong
    comments: true
    date: 2018-06-15 20:27
    layout: post
    title: Papper-XGBoost A Scalable Tree Boosting System
    categories:
    - prml
    tags:
    - prml
    - papper
---



# Abstract

- xgb是可拓展的端到端的提升树，对于稀疏数据进行稀疏感知，用分布式加权直方图算法（weighted quantile sketch）来近似树学习。提供缓存访问模式、数据压缩和共享。
- xgb可以使用比现有系统少得多的资源来拓展数十亿个样本。

# 1.Introduction

- 一些大数据分析的成功的两个原因：
  - 有效的统计模型的应用，获取复杂的数据依赖
  - 可脱产的学习系统，从大数据种学习模型
- 梯度提升树（Boosted Tree）是一种在多个应用中大放异彩的技术，LambdaMART 是用于排序、ctr等场景的提升树变体。
- 比现有的框架快10倍以上，可拓展性高！这是来源于一些创新：
  - 一种新颖的处理稀疏数据的树学习算法
  - 一种分布式加权直方图算法，可以在近似树学习的过程中处理样本权重
  - 并行和分布式计算使得学习速度快。
  - 利用out-of-core计算，针对核外树学习提出了一种有效的缓存感知块结构。

# 2.TREE BOOSTING IN A NUTSHELL（回归树）

## 2.1模型

- 模型：用K个函数来进行预测，每个回归树每个叶子上有连续分数，我们使用wi表示第i个叶子上的分数，通过综合对应树叶的得分（由w给出）来计算最终预测。
  - xi是第i个样本的特征，有m个特征
  - yi是第i个样本的标签
  - F是回归树CART的函数空间
  - q是每个树的结构（决策规则），将一个样本的特征映射到（一些）叶子，并将wi求和。
  - T是树上叶子的数量
  - fk对应于一个树结构q和叶子权重们w
  - wi是第i个叶子节点的score（连续值）


![](http://7xiegr.com1.z0.glb.clouddn.com/xgboost_1.PNG)

![](http://7xiegr.com1.z0.glb.clouddn.com/xgboost_2.PNG)

## 2.2目标函数

- 对正则的objective有小幅改进，用了二阶方法。
- 目标函数1：
  - l是可微分的凸函数
  - Ω是正则项，用于平滑权重，包括：
    - 叶子节点数
    - 叶子权重

![](http://7xiegr.com1.z0.glb.clouddn.com/xgboost_3.PNG)

- 目标函数2：

  - 目标函数1包括函数作为参数，在欧式空间中不能使用传统的优化方法优化，使用加性方式训练。与adaboost相似，假设^ yi（t）是第t次迭代中第i个样本的预测，我们需要加上ft来最小化以下目标：

  ![](http://7xiegr.com1.z0.glb.clouddn.com/xgboost_4.PNG)

  - 由于泰勒展开满足：

  $$
  \begin{split}
  f(x+\Delta x)\simeq f(x)+f'(x)\Delta x+\frac12f''(x)\Delta x^2
  \end{split}
  $$

  - 用泰勒展开来近似目标。二阶近似可以用来快速优化目标，将上式进行二阶泰勒展开：
    - gi是一阶梯度
    - hi是二阶梯度

  ![](http://7xiegr.com1.z0.glb.clouddn.com/xgboost_5.PNG)

  ![](http://7xiegr.com1.z0.glb.clouddn.com/xgboost_6.PNG)

  - 去掉与待求参数无关的常数项，这个新目标函数有一个非常明显的特点，它只依赖于每个数据点的在误差函数上的一阶导数和二阶导数，从而得到新的优化目标为：

  ![](http://7xiegr.com1.z0.glb.clouddn.com/xgboost_7.PNG)

  - 这一个目标函数有一个非常明显的特点，它只依赖于每个数据点的在误差函数上的一阶导数和二阶导数  
  - 将上式变形，将关于样本迭代转换为关于树的叶子节点迭代：
    - 因为每个数据点落入且仅落入一个叶子节点，所以可以把n个数据点按叶子成组，类似于合并同类项，两个数据点同类指的是落入相同的叶子，即这里的指示变量集Ij  
    - 我们如何选择每一轮加入什么f呢？答案是非常直接的，选取一个f来使得我们的目标函数尽量最大地降低 
    - Ij={i|q(xi) = j}是叶子j的样本集合

  ![](http://7xiegr.com1.z0.glb.clouddn.com/xgboost_8.PNG)

- 目标函数3：

  - 对于每个叶子j的wj，有以下计算最优解的公式  

  ![](http://7xiegr.com1.z0.glb.clouddn.com/xgboost_9.PNG)

  - 求权重对应的最优值，该式可以用来衡量树的质量。代表了当我们指定一个树的结构的时候，我们在目标上面最多减少多少（这个分数越小，代表树的结构越好）。我们可以把它叫做结构分数(structure score)，类似于信息增益、基尼系数：

![](http://7xiegr.com1.z0.glb.clouddn.com/xgboost_10.PNG)

- 分裂增益：记分到左子树的样本集为IL,分到右子树的样本集为IR，I=IL 并 IR。分裂该节点的增益如下，这个loss用于选分裂方案候选： 

![](https://images2015.cnblogs.com/blog/754644/201605/754644-20160530170902758-1033686275.jpg)

![](http://www.52cs.org/wp-content/uploads/2015/04/image1.png)

- 我们希望找到一个属性以及其对应的大小，使得上式取值最大。
- 引入分割不一定会使得情况变好，因为我们有一个引入新叶子的惩罚项。优化这个目标对应了树的剪枝， 当引入的分割带来的增益小于一个阀值的时候，我们可以剪掉这个分割。大家可以发现，当我们正式地推导目标的时候，像计算分数和剪枝这样的策略都会自然地出现，而不再是一种因为heuristic而进行的操作了。 

## 2.3算法


- 枚举所有树结构：不能枚举所有树结构，太复杂。

- 贪心算法求分割点。只能用贪心算法来分裂节点， 从单个节点开始，遍历所有属性，遍历属性的可能取值， 并反复将分支添加进来。

  - 精确：列举所有特征上的所有可能分割。必须首先根据特征值对所有样本进行预排序，然后按排序顺序访问数据，以上式中的梯度统计量。大多数现有的gbdt实现都是精确的贪婪算法。但是当数据不能完全载入内存时，就不能做到这一点了。

  ![](http://7xiegr.com1.z0.glb.clouddn.com/xgboost_12.PNG)

  - 近似：根据特征分布的百分位提出候选分裂点，该算法然后将连续特征值映射到由这些候选点分割的桶中，在遍历该特征的时候，只需要遍历各个分位点，从而计算最优划分。 有两种方法：
    - 全局变体：在新生成一棵树之前就对各个特征计算分位点并划分样本，之后在每次分裂过程中都采用近似划分。需要更多候选点。
    - 局部变体：在具体的某一次分裂节点的过程中采用近似算法。适合更深的树。 

  ![](http://7xiegr.com1.z0.glb.clouddn.com/xgboost_13.PNG)

## 2.4收缩和采样

- 收缩：在每棵树加进来后，将新增加的w按因子α缩放，类似于优化中的学习率，收缩减少了每棵树的影响，并为未来的树留出空间来改进模型。
- 特征列采样：也加速了稍后描述的并行算法的计算。

## 2.5带权重直方图算法    

- 候选分裂点的选取通常选取特征的百分位数从而使候选在数据上均匀分布。Dk = {(x1k, h1), (x2k, h2) · · · (xnk, hn)} 代表第k个特征值和每个样本的二阶梯度。可以定义一个rank function如下，代表样本的特征值k比z小的比率：

![](http://7xiegr.com1.z0.glb.clouddn.com/xgboost_14.PNG)

- 希望得到的分位点{sk1, sk2, · · · skl}满足如下条件：

![](http://7xiegr.com1.z0.glb.clouddn.com/xgboost_15.PNG)

- 这意味着大概有1/ϵ个分位点，其中ϵ是一个近似因子。
- 二阶导hi做为第i个样本的权重，*因为目标函数还可以写成带权的形式* ，原理如下：

![](http://7xiegr.com1.z0.glb.clouddn.com/xgboost_16.PNG)

- 为了优化该问题，本文还提出了分布式 *weighted quantile sketch algorithm* ，该算法的优点是解决了带权重的直方图算法问题，以及有理论保证。 

## 2.6Sparsity-aware Split Finding

- 稀疏性可能有多种原因：
  - 数据中存在缺失值
  - 数据有很多0
  - onehot编码
- 给每个树节点加一个默认方向，当稀疏矩阵中一个值缺失时，其样本被分到默认方向。
- 默认方向选取：
  - 主要思想：分别假设特征缺失的样本属于右子树和左子树，而且只在不缺失的样本上迭代，分别计算缺失样本属于右子树和左子树的增益，选择增益最大的方向为缺失数据的默认方向。
  - 算法描述：

![](http://7xiegr.com1.z0.glb.clouddn.com/xgboost_17.PNG)

# 3.系统设计

## 3.1Column Block for Parallel Learning

- 算法中最耗时的部分就是预排序
- XGBoost将数据存在内存单元block中，每个块中的数据都以compressed column (CSC)形式存储，每一列(一个属性列)均升序存放 。
- 在精确贪心算法中，将所有数据均导入一个块中，算法只要在数据中线性扫描已经预排序过的特征就可以。
- 对于近似算法，可以用多个block(Multiple blocks)分别存储不同的样本集，多个block可以并行计算，也可以在硬盘中存储。
- 重要的是，由于将数据按列存储，可以同时访问所有列，那么可以对所有属性同时执行split finding算法，从而并行化split finding。

## 3.2Cache-aware Access    

- 由于样本按特征进行了预排序，样本对应的统计量(一阶和二阶梯度)需要根据行索引来查找，这导致了内存的不连续访问，容易导致cpu cache命中率降低。 
- 对于精确的贪婪算法，我们可以通过Cache-aware Access来缓解这个问题。 

## 3.3Blocks for Out-of-core Computation

- 为了更好地利用计算机的磁盘空间，对于不能一次性导入到内存的数据，我们将数据分成多个block存在磁盘上，在计算过程中，用另外的线程读取数据，但是由于磁盘IO速度太慢，通常跟不上计算的速度。所以，我们采用了下面两种方法有优化速度和存储：
  - Block compression：将block按列压缩，对于行索引，只保存第一个索引值，然后只保存该数据与第一个索引值之差(offset)，一共用16个bits来保存 offset,因此，一个block一般有216个样本。
  - Block sharding：提高磁盘的吞吐量(??)
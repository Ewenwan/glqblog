# 1.研究背景

- 对模型预测精度无明显影响
- 压缩模型的参数数量、深度来降低模型空间复杂度
- 不显著提高训练时间复杂度，降低预测时间复杂度

# 2.方法

## 2.1.更精细模型的设计

- Resnet

## 2.2.模型裁剪

###2.2.1权值共享

### 2.2.2将不重要的connection或者filter进行裁剪

#### 2.2.2.1单个权重(Weight)剪枝——非结构化

- 定义：任意权重被看作是单个参数并进行随机非结构化剪枝。
- [1989,Lecun,NIPS]Optimal brain damage：
  - 思想：将网络中任意参数都看做单个参数
  - 结果：有效提高预测准确率，不能减小运行时间，剪枝代价高，适用于小网络
- [1993,Hassibi,NIPS]Second Order Derivatives for Network Pruning:Optimal Brain Sugeon：
  - 思想：在Optimal brain damage基础上增加了基于手术恢复权重更新的步骤
  - 结果：在准确率、泛化能力上提升，但是计算量大
- [2015,Srinivas,BMVC]Data-free parameter pruning for deep neural networks：
  - 思想：与Optimal Brain Sugeon相似，将剪枝看作是将小权重置零的操作，手术恢复则相当于找到相似权重补偿被置零的权重造成的激活值损失。
  - 结果：在剪枝时完全不依赖任何训练数据，只依赖网络中的权重，因而通过手术恢复准确率时不需要任何训练数据调优，通过大量推导近似大大减少计算复杂度
- 缺点：
  - 导致网络连接不规整，需要通过稀疏表达来减少内存占用，进而导致在前向传播时，需要大量条件判断和额外空间来标明0或非0参数位置，因此不适合并行计算。
  - 非结构化的稀疏性需要使用专门的软件计算库或者硬件。

#### 2.2.2.2核内权重(Intra Kernel Weight)剪枝——结构化

- [2017,Anwar,JETC]Structured pruning of deep convolutional neural networks：
  - 思想1：定义显著性变量并进行贪婪剪枝。核内定步长粒度，将细粒度剪枝转化为粗粒度剪枝。
    - 全连接或稠密连接：一般作用到同一输入特征图上的Kernel必须采用相同的步长和偏置
    - 卷积层不是稠密连接：作用在不同特征图上的kernel步长与偏置可以不同
  - 思想2：使用进化粒子滤波器决定网络连接重要性

#### 2.2.2.3卷积核(Kernel)/特征图(Feature Map)/通道(Channel/ Filter)剪枝——结构化

- 定义：减去第i层的filter，进而减去第i层产生的部分特征图和第i+1层的部分kernel。
- kernel粒度的显著性度量可以采用kernel的权重和来判断
  - 采用Filter的权重和来判断
  - 其他
- FeatureMap粒度的显著性度量确定：
  - 采用Filter权重和来判断
  - 其他
- [2015,Polyak,IEEE Access]Channel-Level Acceleration of Deep Face Representations
  - InboundPruning方法
- [2017,Li,ICLR]Pruning Filters for Efficient ConvNets
  - 全局贪婪剪枝方法
  - 思想：选取filter权重和作为显著性度量，对每一层中的filter从大到小排序，画出权重和关于排序后下标的曲线，若曲线陡峭，则在这一层减去更多filter。在全部剪枝完成后再通过一次训练恢复准确率。
- 优点：
  - 不依赖任何稀疏卷积计算库及专用软件
  - 能大大减少测试的计算时间。

#### 2.2.2.4中间隐层(Layer)剪枝

- 定义：删除一些层



- Pruning Filters for Efficient Convnets
- An Entropy-based Pruning Method for CNN Compression
- Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning
- Coarse Pruning of Convolutional Neural Networks with Random Masks
- Efficient Gender Classification Using a Deep LDA-Pruned Net
- Sparsifying Neural Network Connections for Face Recognition
- ​

## 2.3其他方法

- [2017,Hu,ICLR]Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures
  - 思想：基于统计的方法删除对大部分不同输入都输出零值的单元，并进行交替重新训练
- [2017,Molchanov,ICLR]Pruning Convolutional Neural Networks for Resource Efficient Transfer Learning Inference
  - 思想：结合贪婪剪枝和基于反向传播的微调来确保剪枝后的网络的泛化性。提出了一种基于泰勒展开来近似计算去除部分参数后网络的损失函数的变化
- [2017,Rueda,GCRP]Neron Pruning for Compressing Deep Networks using Maxout Architectures
  - 思想：提出一种最大化输出单元将多个神经元合并为更复杂的凸函数表达，并根据各个神经元再训练集上的响应的局部相关性进行选择。

## 2.3.核的稀疏化

- 对权重的更新加以正则项进行诱导，使其更加稀疏，使大部分的权值都为0。

### 2.3.1regular

- regular的稀疏化后，裁剪起来更加容易，尤其是对im2col的矩阵操作，效率更高
- Learning Structured Sparsity in Deep Neural Networks

### 2.3.2irregular

- irregular的稀疏化后，参数需要特定的存储方式，或者需要平台上稀疏矩阵操作库的支持
- Dynamic Network Surgery for Efficient DNNs
- Training Skinny Deep Neural Networks with Iterative Hard Thresholding Methods

## 2.4.量化

## 2.5.网络分解

### 2.5.1编码

### 2.5.2低秩分解

## 2.6.迁移学习/网络精馏

# 参考资料

- https://blog.csdn.net/wspba/article/details/75671573
- [[1989,Papper]Optimal Brain Damage（OBD）](http://papers.nips.cc/paper/250-optimal-brain-damage.pdf)
- [2015,Deep Compression](https://arxiv.org/abs/1510.00149)
- ​
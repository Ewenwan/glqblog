# 1.研究背景

- 对模型预测精度无明显影响
- 压缩模型的参数数量、深度来降低模型空间复杂度
  - 模型大小由全连接层主导
- 不显著提高训练时间复杂度，降低预测时间复杂度（计算量）
  - 计算代价由卷积操作主导

# 2.方法

## 2.1.更精细模型的设计

- Aggregated Residual Transformations for Deep Neural Networks
- ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices
- SqueezeNet
- MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications

## 2.2.模型裁剪

### 2.2.1将不重要的connection或者filter进行剪枝

- 思路源头都是来自于Oracle pruning 的方法，即挑选出模型中不重要的参数，将其剔除而不会对模型的效果造成太大的影响，而如何找到一个有效的对参数重要性的评价手段，在这个方法中就尤为重要，我们也可以看到，这种评价标准花样百出，各有不同，也很难判定那种方法更好。在剔除不重要的参数之后，通过一个retrain的过程来恢复模型的性能，这样就可以在保证模型性能的情况下，最大程度的压缩模型参数及运算量。目前，基于模型裁剪的方法是最为简单有效的模型压缩方式。

#### 2.2.1.1单个权重(Weight)剪枝——非结构化

- 定义：任意权重被看作是单个参数并进行随机非结构化剪枝。
- [1989,Lecun,NIPS]Optimal brain damage：
  - 思想：将网络中任意参数都看做单个参数
  - 结果：有效提高预测准确率，不能减小运行时间，剪枝代价高，适用于小网络
- [1993,Hassibi,NIPS]Second Order Derivatives for Network Pruning:Optimal Brain Sugeon：
  - 思想：在Optimal brain damage基础上增加了基于手术恢复权重更新的步骤
  - 结果：在准确率、泛化能力上提升，但是计算量大
- [2015,Srinivas,BMVC]Data-free parameter pruning for deep neural networks：
  - 思想：与Optimal Brain Sugeon相似，将剪枝看作是将小权重置零的操作，手术恢复则相当于找到相似权重补偿被置零的权重造成的激活值损失。
  - 结果：在剪枝时完全不依赖任何训练数据，只依赖网络中的权重，因而通过手术恢复准确率时不需要任何训练数据调优，通过大量推导近似大大减少计算复杂度
- [2015,Han,NIPS]Learning both Weights and Connections for Efficient Neural Network
  - 思想：修剪训练后网络中的不重要连接（connections），来减少网络所需要的参数。
- [2016,Kadetotad,ICCAD]Efficient memory compression in deep neural networks using coarse-grain sparsification for speech applications
- [2017,Molchanov,ICLR]Pruning Convolutional Neural Networks for Resource Efficient Transfer Learning Inference
  - 思想：从众多的权重参数中选择一个最优的组合B，使得被裁剪的模型的代价函数的损失最小。结合贪婪剪枝和基于反向传播的微调来确保剪枝后的网络的泛化性。提出了一种基于泰勒展开来近似计算去除部分参数后网络的损失函数的变化。
- 缺点：
  - 导致网络连接不规整，需要通过稀疏表达来减少内存占用，进而导致在前向传播时，需要大量条件判断和额外空间来标明0或非0参数位置，因此不适合并行计算。
  - 非结构化的稀疏性需要使用专门的软件计算库或者硬件。

#### 2.2.1.2核内权重(Intra Kernel Weight)剪枝/核的稀疏化——结构化

- 对权重的更新加以正则项进行限制，使其更加稀疏，使大部分的权值都为0。
- regular
  - regular的稀疏化后，裁剪起来更加容易，尤其是对im2col的矩阵操作，效率更高
  - Learning Structured Sparsity in Deep Neural Networks
- irregular
  - irregular的稀疏化后，参数需要特定的存储方式，或者需要平台上稀疏矩阵操作库的支持
  - Dynamic Network Surgery for Efficient DNNs
  - Training Skinny Deep Neural Networks with Iterative Hard Thresholding Methods

- [2017,Anwar,JETC]Structured pruning of deep convolutional neural networks：
  - 思想1：定义显著性变量并进行贪婪剪枝。核内定步长粒度，将细粒度剪枝转化为粗粒度剪枝。
    - 全连接或稠密连接：一般作用到同一输入特征图上的Kernel必须采用相同的步长和偏置
    - 卷积层不是稠密连接：作用在不同特征图上的kernel步长与偏置可以不同
  - 思想2：使用进化粒子滤波器决定网络连接重要性

#### 2.2.1.3卷积核(Kernel)/特征图(Feature Map)/通道(Channel/ Filter)剪枝——结构化

- 定义：减去第i层的filter，进而减去第i层产生的部分特征图和第i+1层的部分kernel。
- kernel粒度的显著性度量可以采用kernel的权重和来判断
  - 采用Filter的权重和来判断
  - 其他
- FeatureMap粒度的显著性度量确定：
  - 采用Filter权重和来判断
  - 其他
- [2015,Polyak,IEEE Access]Channel-Level Acceleration of Deep Face Representations
  - InboundPruning方法
- [2017,Li,ICLR]Pruning Filters for Efficient ConvNets
  - 全局贪婪剪枝方法
  - 思想：选取filter权重和作为显著性度量，对每一层中的filter从大到小排序，画出权重和关于排序后下标的曲线，若曲线陡峭，则在这一层减去更多filter。在全部剪枝完成后再通过一次训练恢复准确率。
- [2017,Hu,ICLR]Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures
  - 思想：激活为0的神经元是冗余的，基于统计的方法删除对大部分不同输入都输出零值的单元，并进行交替重新训练
- An Entropy-based Pruning Method for CNN Compression
  - 思想：作者认为通过weight值的大小很难判定filter的重要性，通过这个来裁剪的话有可能裁掉一些有用的filter。因此作者提出了一种基于熵值的裁剪方式，利用熵值来判定filter的重要性。
- Designing Energy-Efficient Convolutional Neural Networks using Energy-Aware Pruning
  - 作者指出一个模型中的能量消耗包含两个部分，一部分是计算的能耗，一部分是数据转移的能耗，在作者之前的一片论文中（与NVIDIA合作，Eyeriss），提出了一种估计硬件能耗的工具，能够对模型的每一层计算它们的能量消耗。然后将每一层的能量消耗从大到小排序，对能耗大的层优先进行裁剪，这样能够最大限度的降低模型的能耗，对于需要裁剪的层，根据weight的大小来选择不重要的进行裁剪，同样的作者也考虑到不正确的裁剪，因此将裁剪后模型损失最大的weight保留下来。 
- Coarse Pruning of Convolutional Neural Networks with Random Masks
  - 思想：既然无法直观上的判定filter的重要性，那么就采取一种随机裁剪的方式，然后对于每一种随机方式统计模型的性能，来确定局部最优的裁剪方式。 这种随机裁剪方式类似于一个随机mask，假设有M个潜在的可裁剪weight，那么一共就有2^M个随机mask。假设裁剪比例为a，那么每层就会随机选取ML*a个filter，一共随机选取N组组合，然后对于这N组组合，统计裁剪掉它们之后模型的性能，然后选取性能最高的那组作为局部最优的裁剪方式。
  - 作者同样将这种裁剪方式运用在kernel上，对kernel进行裁剪，并将filter级和kernel级的裁剪进行组合，得到了较好的结果。 
- Efficient Gender Classification Using a Deep LDA-Pruned Net
  - 思想：作者发现，在最后一个卷积层中，经过LDA分析发现对于每一个类别，有很多filter之间的激活是高度不相关的，因此可以利用这点来剔除大量的只具有少量信息的filter而不影响模型的性能
- Sparsifying Neural Network Connections for Face Recognition
  - 思想：本文应用场景为人脸识别的模型DeepID。作者认为，如果一层中的某个神经元的激活与上一层的某个神经元的激活有很强的相关性，那么这个神经元对于后面层的激活具有很强的判别性。也就是说，如果前后两层中的某对神经元的激活具有较高的相关性，那么它们之间的连接weight就是非常重要的，而弱的相关性则代表低的重要性。如果某个神经元可以视为某个特定视觉模式的探测器，那么与它正相关的神经元也提供了这个视觉模式的信息，而与它负相关的神经元则帮助减少误报。作者还认为，那些相关性很低的神经元对，它们之间的连接不一定是一点用也没有，它们可能是对于高相关性神经元对的补充。
- 优点：
  - 不依赖任何稀疏卷积计算库及专用软件
  - 能大大减少测试的计算时间。

#### 2.2.1.4中间隐层(Layer)剪枝

- 定义：删除一些层
- ​




###2.2.2权值共享

#### 2.2.2.1哈希

- Functional Hashing for Compressing Neural Networks. (Baidu Inc)
- [2015,Chen,ICML]Compressing Neural Networks with the Hashing Trick
  - 思想：将参数映射到相应的哈希桶内，在同一个哈希桶内的参数则共享同一值。

#### 2.2.2.2定点

- [2011,Vanhoucke,NIPS]Improving the speed of neural networks on CPUs
- [2014,Hwang,Journal of Signal Processing Systems]Fixed-point feedforward deep neural network design using weights+1,0,and -1


#### 2.2.2.3其他

- [2014,Gong,]Compressing deep convolutional networks using vector quantization
  - 思想：通过使用kmeans算法将全部参数进行聚类，每簇中参数共享中心值。
- Learning compact recurrent neural networks. (University of Southern California + Google)

### 2.2.3结构化矩阵

- 思想：如果一个 m x n 阶矩阵只需要少于 m×n 个参数来描述，就是一个结构化矩阵。通常这样的结构不仅能减少内存消耗，还能通过快速的矩阵-向量乘法和梯度计算显著加快推理和训练的速度。
- Structured Convolution Matrices for Energy-efficient Deep learning. (IBM Research–Almaden)
- Structured Transforms for Small-Footprint Deep Learning. (Google Inc)
- An Exploration of Parameter Redundancy in Deep Networks with Circulant Projections.
- Theoretical Properties for Neural Networks with Weight Matrices of Low Displacement Rank.


## 2.3.迁移学习/网络精馏

- 迁移学习：将一个模型的性能迁移到另一个模型上
- 网络精馏：在同一个域上迁移学习的一种特例。
- [2006,Bucila,SIGKDD]Model Compression
- [2014,Hinton,NIPS]Distilling the Knowledge in a Neural Network
  - 思想：这个复杂的网络是提前训练好具有很好性能的网络，学生网络的训练含有两个目标：一个是hard target，即原始的目标函数，为小模型的类别概率输出与label真值的交叉熵；另一个为soft target，为小模型的类别概率输出与大模型的类别概率输出的交叉熵，在soft target中，概率输出的公式调整如下，这样当T值很大时，可以产生一个类别概率分布较缓和的输出。作者认为，由于soft target具有更高的熵，它能比hard target提供更加多的信息，因此可以使用较少的数据以及较大的学习率。将hard和soft的target通过加权平均来作为学生网络的目标函数，soft target所占的权重更大一些。 作者同时还指出，T值取一个中间值时，效果更好，而soft target所分配的权重应该为T^2，hard target的权重为1。 这样训练得到的小模型也就具有与复杂模型近似的性能效果，但是复杂度和计算量却要小很多。 
- [2014,Ba,NIPS]Do deep nets really need to be deep
  - 思想：设计了更浅却更宽的学生模型，同时保证两者的网络参数相同。
- [2017,Romero,ICLR]Hints for the deep nets
  - 思想：设计了更深的更窄的学生模型，同时保证两者的网络参数相同，采用Hints方式。
- [2016,Chen,ICLR]Net2net:Accelerating learning via knowledge transfer
  - 思想：一种网络生长的方法来获得学生模型的网络结构
- [2016,Li,ECCV]Learning without forgetting
  - 思想：分别从宽度和深度上进行网络生长，然后利用网络蒸馏方法训练学生模型
- Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer
  - 思想：使用复杂网络中能够提供视觉相关位置信息的Attention map来监督小网络的学习，并且结合了低、中、高三个层次的特征。教师网络从三个层次的Attention Transfer对学生网络进行监督。其中三个层次对应了ResNet中三组Residual Block的输出。在其他网络中可以借鉴。 这三个层次的Attention Transfer基于Activation，Activation Attention为feature map在各个通道上的值求和。
- [2016,Sau,]Deep model compression:distilling knowledge from noisy teachers
- Sequence-Level Knowledge Distillation. (Harvard University)
- Like What You Like: Knowledge Distill via Neuron Selectivity Transfer. (TuSimple)

## 2.4.网络分解

### 2.4.1编码

- [2016,Han,ICLR]Deep compression:compressing deep neural networks with pruning,trained quantization and Huffman coding
  - 思想：首先修剪不重要的连接，重新训练稀疏连接的网络。然后使用权重共享量化连接的权重，再对量化后的权重和码本（codebook）使用霍夫曼编码，以进一步降低压缩率。

### 2.4.2卷积核的低秩分解/矩阵分解

- 思路：
  - 将三维张量铺展成二维张量，使用标准SVD方法
  - 使用多个一维张量外积求和逼近（用三个一维张量外积求和进行K次累加来逼近一个秩为K的三维张量）
- [2015,Jaderberg,BMVC]Speeding up convolutional neural networks with low rank expansions
  - 思想：使用秩为1（可以分解为行向量与列向量乘积）的卷积核作用在输入图上产生相互独立的M个基本特征图，然后通过学习到的字典权重利用线性组合重构出输出特征图。
- [2015,Liu,CVPR]Sparse convolutional neural networks
  - 思想：利用通道间和通道内冗余，在通道上进行稀疏分解，然后将高计算代价的卷积操作转换成矩阵相乘。然后再将矩阵稀疏化，结合微调步骤，最小化含有稀疏性最大化正则项的损失函数
- [2015,Kim,Computer science]Compression of deep convolutional neural networks for fast and low power mobile applications
  - 思想：首先进行变分贝叶斯矩阵分解的秩选择，然后再进行核张量Tucker分解，最后再次对模型进行调整。
- [2013,Sainath,ieeeicassp]Low-rank matrix factorization for deep neural network training with high-dimensional output targets
  - 思想：
- [2013,Denil,NIPS]Predicting Parameters in Deep Learning
  - 思想：
- [2015,Nakkiran,ISCA]Compressing Deep Neural Networks Using a Rank-Constrained Topology
- [2014,Denton,NIPS]Exploiting linear structure within convolutional netowrks for efficient evalution

## 2.5权重量化

- 思想：通过减少表示每个权重所需的比特数来压缩原始网络。


- [2016,Wu,CVPR]Quantized convolutional neural networks for mobile devices
  - 思想：对参数值使用 K 均值标量量化。
- [2015,Courbariaux,NIPS]Training deep neural networks with binary weights during propagations
  - 思想：二值化的权重策略对网络的权重进行较为极端的量化，限制权重只能是-1或1。
- [2017,Zhou,ICLR]Incremental network quantization:Towards lossless cnns with low-precision weights
  - 思想：增量式网络量化方法，三种独立操作：权重划分、分组量化、再训练。
- [2016,Soulie,ICANN]Compression of deep neural networks on the fly
  - 思想：在模型学习阶段进行压缩的方法。首先在全连接层损失函数上增加额外的归一项，使得权重趋向于二进制值，然后对输出层进行粗粒度的量化。
- [2015,]Deep learning with limited numerical precision
  - 思想：基于随机修约（stochastic rounding）的 CNN 训练中使用 16 比特定点表示法（fixed-point representation），显著降低内存和浮点运算，同时分类准确率几乎没有受到损失。
- 效果：权重量化的方法比稀疏分解效果更好。

## 2.6其他方法

- [2017,Rueda,GCRP]Neron Pruning for Compressing Deep Networks using Maxout Architectures
  - 思想：提出一种最大化输出单元将多个神经元合并为更复杂的凸函数表达，并根据各个神经元再训练集上的响应的局部相关性进行选择。

# 参考资料

- https://blog.csdn.net/wspba/article/details/75671573
- https://www.ctolib.com/ZhishengWang-Embedded-Neural-Network.html
- A Survey of Model Compression and Acceleration for Deep Neural Networks
---
    author: LuckyGong
    comments: true
    date: 2018-01-01 15:27
    layout: post
    title: 词的向量表示
    categories:
    - nlp
    tags:
    - nlp
    - prml
---

# 1.one-hot encoding（词袋模型，最传统方式）

- 定义：表示一个单词：只有一维是1 ，其他维是0。
- 缺点：语义鸿沟（同义词问题，one-hot没有语义）、维度灾难、稀疏、无法表示未出现在词表中的词、增加新词麻烦。

# 2.count-based

## 2.1 基于词频统计

one-hot的增强版，引入了词频。

## 2.2 tf*idf

- **定义：**引入了逆文档频率。
  - **tf（词频）：**一词语出现的次数除以该文档的总词语数。
  - **idf（逆文档频率）：**文档频率。一词语出现在多少文档的数量除以总文档数。
- **假设**：如果某个词在一篇文章中出现的频率高，并且在其他文章中很少出，那么它很可能就反映了这篇文章的特性，因此要提高它的权值。

  ​

# 3.分布式表示

## 3.1 原理

- **定义：**将one-hot压缩到低维空间，每一维可以看成词的语义或主题信息，语义相似的词语距离近。
- **优点：**维度压缩、语义提取解决语义鸿沟、基于学习模型可以对未出现在词表中的词进行表示。
- **方法：**LDA、Deep Learning。
- **核心假设：**具有相似上下文信息的词应该具有相似的词表示。
- **词关系：**Paradigmatic（同义词）和Syntagmatic（搭配出现）
- Distributional Representation VS Distributed Representation：
  - Distributional Representation是从分布式假设（由Harris在1954年提出，出现在相同上下文的词语语义相似）的角度，是一类获取词表示的方法。
  - 而Distributed Representation指的是文本表示的形式，就是低维、稠密的连续向量

## 3.2 传统矩阵方法——语言模型

- 如Glove

## 3.3传统矩阵方法——SVD



## 3.4传统聚类方法——

## 3.5 深度学习方法——神经网络语言模型（NNLM）

## 3.6 深度学习方法——CBOW/Skipgram（word2vec）

- NNLM方法的升级版。
- 去除隐藏层。
- 去不考虑词序：汉字顺序并不影响阅读。

### 3.6.1 连续词袋模型（CBOW）

#### 3.6.1.1思想

- 根据**中心词W(t)周围的词**来预测中心词，BOW的升级版。

#### 3.6.1.2模型

![](https://pic2.zhimg.com/80/v2-0f439e1bb44c71c8e694cc65cb509263_hd.jpg)

- 确定参数：
  - 词典中词汇数目V：
    - 训练数据中所有词：会出现长尾现象
    - 前V个频率最高的词
  - 词向量的维数N：超参数，无原则指导，人工设定，一般几十到几百
- 输入层：有滑动窗口大小个神经元，当前词t前面的c个词以及后面的c个词的one-hot词向量。
- 输入层到隐藏层的权重W：全链接。本质上有很多个W矩阵，每个矩阵就代表了这个单词对隐层的影响和贡献。输入层所有神经元共享权重矩阵W（V*N），随机初始化权重矩阵W。
- 隐藏层：将输入层的词向量加和平均作为隐藏层的输入（也可以是多个隐藏层），乘以权重后得到隐藏层输出向量大小为1*N，这个就是目前的词向量。
- 隐藏层到输出层的权重W‘：全链接，W’（N*V）
- 输出层：将隐藏层输出向量乘以矩阵W' ，得到向量1*V，激活函数处理得到V-dim概率分布，其概率最大的词为中心词。、

#### 3.6.1.3策略

交叉熵代价函数

#### 3.6.1.4算法

- 采用梯度下降算法更新W和W'。
- 训练完毕后，输入层的每个单词与矩阵W相乘得到的向量的就是我们想要的词向量（word embedding），这个矩阵（所有单词的word embedding）也叫做look up table（其实这个look up table就是矩阵W自身），也就是说，任何一个单词的onehot乘以这个矩阵都将得到自己的词向量。有了look up table就可以免去训练过程直接查表得到单词的词向量了。

#### 3.6.1.5评价

- CBOW对小型数据库比较合适



### 3.6.2 skip-gram

#### 3.6.2.1思想

- 这个模型的建立与连续词袋模型（CBOM）非常相似，但本质上是交换了输入和输出的位置。
- 根据**中心词W(t)**来预测周围词

#### 3.6.2.2模型

![](http://i.stack.imgur.com/igSuE.png)

- 将该单词生成one-hot输入向量（1×V）
- 乘以权重W（V×N），得到该单词的上下文的嵌入词向量（1×N）
- 乘以W‘（N×V），得到向量（1×V），送给softmax，得到V个概率，找出C个最大的，就是其周围的。

#### 3.6.2.3算法

- 采用skip-gram策略用SGD算法优化

#### 3.6.2.4评价

- Skip-Gram在大型语料中表现更好。 

## 3.7 实现工具

- word2vec
- gensim
- fasttext

# 4.参考资料

1. word2vec是如何得到词向量的？ - crystalajj的回答 - 知乎

   https://www.zhihu.com/question/44832436/answer/266068967

2. https://www.leiphone.com/news/201706/PamWKpfRFEI42McI.html

3. http://blog.csdn.net/han_xiaoyang/article/details/51567822

4. http://www.cnblogs.com/peghoty/p/3857839.html